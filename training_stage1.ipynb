{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Average, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Pre-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "PRESAVED_MODEL = \"/kaggle/input/jmt-mlm-pretrain-128/\"\n",
    "MODEL_PATH = r\"/kaggle/input/jmt-pretrain-tot-lang-info-v2/\"\n",
    "TOKENIZER_MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "DATA_PATH = \"/kaggle/input/jmt-preprocess-lang-info-tta-v2/\"\n",
    "DATA_AUG_PATH = \"/kaggle/input/nlp-albumentations-xl-v3-test/\"\n",
    "OPUS_PATH = \"/kaggle/input/jmt-data-module-opus/\"\n",
    "SEED = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([36296., 11188.,  6428.,  3521.,  2660.,  1534.,  1041.,   790.,\n",
       "          308.,    46.]),\n",
       " array([0.00475234, 0.07783199, 0.15091164, 0.22399129, 0.29707094,\n",
       "        0.37015059, 0.44323024, 0.51630988, 0.58938953, 0.66246918,\n",
       "        0.73554883]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl_sub =  pd.read_csv('/kaggle/input/tot-blend-for-pl-0615/submission.csv')\n",
    "plt.hist(pl_sub.toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15000313420673228\n",
      "63812\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = np.percentile(pl_sub.toxic, 85)\n",
    "y_test = (pl_sub.toxic.values>THRESHOLD).astype('int')\n",
    "print(np.mean(y_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_test = test['lang'].values\n",
    "lang_wt = np.zeros(len(test))\n",
    "lang_wt[lang_test == 'it'] = 0.7\n",
    "lang_wt[lang_test == 'es'] = 0.8\n",
    "lang_wt[lang_test == 'pt'] = 0.9\n",
    "lang_wt[lang_test == 'fr'] = 1.0\n",
    "lang_wt[lang_test == 'ru'] = 1.1\n",
    "lang_wt[lang_test == 'tr'] = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Load Datasets from Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1.96 s, total: 1.96 s\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train = np.load(os.path.join(DATA_PATH, 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(DATA_PATH, 'y_train.npy'))\n",
    "x_valid = np.load(os.path.join(DATA_PATH, 'x_valid.npy'))\n",
    "y_valid = np.load(os.path.join(DATA_PATH, 'y_valid.npy'))\n",
    "x_test = np.load(os.path.join(DATA_PATH, 'x_test.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ROC Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class RocAucCallback(Callback):\n",
    "    def __init__(self, valid, valid_ds):\n",
    "        self.valid = valid\n",
    "        self.valid_ds = valid_ds\n",
    "        # self.test_pred = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(roc_auc_score(self.valid['toxic'], self.model.predict(self.valid_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start=1e-5, lr_max=2e-5, \n",
    "               lr_min=1e-6, lr_rampup_epochs=5,\n",
    "               lr_sustain_epochs=0, lr_exp_decay=.8):\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "    \n",
    "    return lrfn"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.2):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, loss='binary_crossentropy', max_len=512):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "\n",
    "    mean_token = GlobalAveragePooling1D()(sequence_output)\n",
    "    max_token = GlobalMaxPooling1D()(sequence_output)\n",
    "    all_token = Concatenate(name = \"all_token\")([mean_token, max_token])\n",
    "    all_token = Dropout(0.3)(all_token)\n",
    "    out = Dense(1, activation='sigmoid')(all_token)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=6e-6), loss=loss, metrics=[tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "early_stop_cb = EarlyStopping(monitor='val_auc', \n",
    "               mode='max',\n",
    "               restore_best_weights=True, \n",
    "               verbose=2,\n",
    "               patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading model weights \n",
      "\n",
      "load weights done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    transformer_layer = TFAutoModel.from_pretrained(PRESAVED_MODEL)\n",
    "    model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=MAX_LEN)\n",
    "    \n",
    "print(\"\\n Loading model weights \\n\")\n",
    "\n",
    "model.load_weights(os.path.join(MODEL_PATH, \"model.h5\"))\n",
    "print(\"load weights done\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### PL on aug test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_aug = np.load(os.path.join(DATA_AUG_PATH, 'x_test.npy'))\n",
    "# assign language code\n",
    "x_test_aug[:, 2:] = x_test_aug[:, 1:-1]\n",
    "x_test_aug[:, 1] = x_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,   4448,  52625, ...,      1,      1,      1],\n",
       "       [     0,    882,    417, ...,      1,      1,      1],\n",
       "       [     0,    217, 132554, ...,      1,      1,      1],\n",
       "       ...,\n",
       "       [     0,    217,   2083, ...,      1,      1,      1],\n",
       "       [     0,   4448, 191059, ...,      1,      1,      1],\n",
       "       [     0,     90,   1413, ...,      1,      1,      1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,  4448,    40, ...,     1,     1,     1],\n",
       "       [    0,   882,   417, ...,     1,     1,     1],\n",
       "       [    0,   217,  1215, ...,     1,     1,     1],\n",
       "       ...,\n",
       "       [    0,   217,  2083, ...,     1,     1,     1],\n",
       "       [    0,  4448,  1945, ...,     1,     1,     1],\n",
       "       [    0,    90, 89412, ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15000313420673228\n"
     ]
    }
   ],
   "source": [
    "y_test_aug = y_test.copy()\n",
    "print(np.mean(y_test_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample opus and merge\n",
    "x_test_pl = np.concatenate((x_train[:120000],x_test,x_test_aug),axis=0)\n",
    "y_test = np.concatenate((y_train[:120000],y_test,y_test_aug),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_test = np.concatenate((np.ones(len(x_train)), lang_wt, lang_wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "indices = np.arange(x_test_pl.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_test_pl = x_test_pl[indices]\n",
    "y_test = y_test[indices]\n",
    "weight_test = weight_test[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247624, 192)\n",
      "(247624,)\n",
      "(247624,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test_pl.shape)\n",
    "print(y_test.shape)\n",
    "print(weight_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "\n",
    "test_pl_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_test_pl, y_test, weight_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-swa\r\n",
      "  Downloading keras-swa-0.1.5.tar.gz (77 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.9 MB/s \r\n",
      "\u001b[?25hBuilding wheels for collected packages: keras-swa\r\n",
      "  Building wheel for keras-swa (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-swa: filename=keras_swa-0.1.5-py3-none-any.whl size=9712 sha256=9bbff06a75f249b57163325e26b1615388236ac901c1bcf892de946178e25da1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/fa/17/c0a0e1d12843447b81a19a262ae416d0f7e08e604d5abbd3fe\r\n",
      "Successfully built keras-swa\r\n",
      "Installing collected packages: keras-swa\r\n",
      "Successfully installed keras-swa-0.1.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-swa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def build_cosine_annealing_lr(eta_min=3e-6, eta_max=1e-5, T_max=10):\n",
    "    def lrfn(epoch):\n",
    "        lr = eta_min + (eta_max - eta_min) * (1 + math.cos(math.pi * epoch / T_max)) / 2\n",
    "        return lr\n",
    "    return lrfn\n",
    "_lrfn = build_cosine_annealing_lr(eta_min=3e-6, eta_max=9e-6, T_max=2)\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(_lrfn, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swa.tfkeras import SWA\n",
    "swa_cb = SWA(start_epoch=5, \n",
    "          lr_schedule='manual', \n",
    "          swa_lr=3e-6,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 9e-06.\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 91s 711ms/step - loss: 0.0223 - auc: 0.9768 - lr: 9.0000e-06 - val_loss: 0.0368 - val_auc: 0.9413\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 6e-06.\n",
      "Epoch 2/15\n",
      "128/128 [==============================] - 60s 466ms/step - loss: 0.0209 - auc: 0.9801 - lr: 6.0000e-06 - val_loss: 0.0343 - val_auc: 0.9439\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 3e-06.\n",
      "Epoch 3/15\n",
      "128/128 [==============================] - 60s 467ms/step - loss: 0.0197 - auc: 0.9827 - lr: 3.0000e-06 - val_loss: 0.0368 - val_auc: 0.9426\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 5.999999999999999e-06.\n",
      "Epoch 4/15\n",
      "128/128 [==============================] - 60s 468ms/step - loss: 0.0194 - auc: 0.9826 - lr: 6.0000e-06 - val_loss: 0.0397 - val_auc: 0.9420\n",
      "\n",
      "Epoch 00005: starting stochastic weight averaging\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 9e-06.\n",
      "Epoch 5/15\n",
      "128/128 [==============================] - 68s 531ms/step - loss: 0.0202 - auc: 0.9813 - lr: 9.0000e-06 - val_loss: 0.0424 - val_auc: 0.9399\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 6.000000000000001e-06.\n",
      "Epoch 6/15\n",
      "128/128 [==============================] - 67s 520ms/step - loss: 0.0193 - auc: 0.9829 - lr: 6.0000e-06 - val_loss: 0.0397 - val_auc: 0.9404\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 3e-06.\n",
      "Epoch 7/15\n",
      "128/128 [==============================] - 65s 510ms/step - loss: 0.0179 - auc: 0.9853 - lr: 3.0000e-06 - val_loss: 0.0465 - val_auc: 0.9379\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 5.9999999999999985e-06.\n",
      "Epoch 8/15\n",
      "128/128 [==============================] - 66s 514ms/step - loss: 0.0176 - auc: 0.9862 - lr: 6.0000e-06 - val_loss: 0.0500 - val_auc: 0.9272\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 9e-06.\n",
      "Epoch 9/15\n",
      "128/128 [==============================] - 66s 513ms/step - loss: 0.0180 - auc: 0.9857 - lr: 9.0000e-06 - val_loss: 0.0531 - val_auc: 0.9270\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 6.000000000000002e-06.\n",
      "Epoch 10/15\n",
      "128/128 [==============================] - 66s 515ms/step - loss: 0.0181 - auc: 0.9856 - lr: 6.0000e-06 - val_loss: 0.0474 - val_auc: 0.9313\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 3e-06.\n",
      "Epoch 11/15\n",
      "128/128 [==============================] - 65s 509ms/step - loss: 0.0168 - auc: 0.9878 - lr: 3.0000e-06 - val_loss: 0.0452 - val_auc: 0.9340\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 5.999999999999993e-06.\n",
      "Epoch 12/15\n",
      "128/128 [==============================] - 66s 513ms/step - loss: 0.0173 - auc: 0.9873 - lr: 6.0000e-06 - val_loss: 0.0549 - val_auc: 0.9205\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 9e-06.\n",
      "Epoch 13/15\n",
      "128/128 [==============================] - 65s 508ms/step - loss: 0.0169 - auc: 0.9872 - lr: 9.0000e-06 - val_loss: 0.0520 - val_auc: 0.9224\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 5.999999999999997e-06.\n",
      "Epoch 14/15\n",
      "128/128 [==============================] - 66s 512ms/step - loss: 0.0169 - auc: 0.9871 - lr: 6.0000e-06 - val_loss: 0.0462 - val_auc: 0.9310\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 3e-06.\n",
      "Epoch 15/15\n",
      "128/128 [==============================] - 66s 516ms/step - loss: 0.0163 - auc: 0.9886 - lr: 3.0000e-06 - val_loss: 0.0454 - val_auc: 0.9337\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2454737d90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "n_steps = x_test_pl.shape[0] // BATCH_SIZE // EPOCHS\n",
    "model.fit(\n",
    "    test_pl_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[swa_cb, lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 16s 256ms/step\n",
      "valid auc 0.9411753791836293\n"
     ]
    }
   ],
   "source": [
    "valid_pred_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_valid)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "valid['pred'] = model.predict(valid_pred_dataset, verbose=1)\n",
    "## early stop didn't save best model\n",
    "print(\"valid auc\", roc_auc_score(valid['toxic'], valid['pred']))\n",
    "\n",
    "valid.to_csv('valid_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/499 [==============================] - 63s 126ms/step\n"
     ]
    }
   ],
   "source": [
    "test_pred_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "test['pred'] = model.predict(test_pred_dataset, verbose=1)\n",
    "test.to_csv('test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.98 s, sys: 3.62 s, total: 5.6 s\n",
      "Wall time: 8.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL_NAME = \"model.h5\"\n",
    "model.save_weights(\"model.h5\")\n",
    "## TO LOAD:\n",
    "# with strategy.scope():\n",
    "#     model2 = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "#     model2.load_weights(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
